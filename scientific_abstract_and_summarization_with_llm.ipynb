{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing research papers with LLM, serpapi and Python ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "It is proposed to create an agent that receives a Bibtex file as input and returns a text report summarizing the documents, mentioning the most relevant documents, researchers, and institutions related to the topic of the summarized documents.\n",
    "\n",
    "# Approach to the Problem:\n",
    "\n",
    "The problem was understood as a linear sequence of steps:\n",
    "1. Input Bibtex file\n",
    "2. Process file and extract titles\n",
    "3. Download corresponding documents for each title\n",
    "4. Summarize each document\n",
    "5. Obtain topics for each document\n",
    "6. For each topic, obtain the most relevant articles, authors, and institutions related to the topic\n",
    "7. Report\n",
    "\n",
    "# Decisions Taken for Problem Solution:\n",
    "\n",
    "1. PDF documents only. A more complete solution should consider other alternatives.\n",
    "2. Each document will have a single topic to facilitate formulation of the search in Google Scholar. A more complete solution should consider documents with multiple topics, like article compilations.\n",
    "3. The authority source for retrieving relevant authors, articles, and institutions will be Google Scholar's ranking. This implies that those who sign the most relevant articles for a given topic are also the most relevant authors, and the same applies to their affiliated institutions. This is a simplification that may be excessive depending on the discipline. A more complete solution should apply reputation studies, analysis of practice communities, among other scientometrics methodologies. Depending on the discipline, other types of institutions, such as regulatory ones, should also be considered.\n",
    "4. The final report will be plain text, without formatting. In a more complete solution, templates should be considered for formatting the text and enhancing the readability of the report.\n",
    "5. The LangChain library will be used. This library became the source of learning about agents and chains, as well as the main toolbox. This led to the most difficult decision: not coding an agent in the strict sense and as understood by LangChain. An agent, according to the authors of the library, has a non-linear workflow, where the agent determines the course of action and chooses which tools to use among those available. Understanding the problem as a linear process no longer fits the workflow of an agent.\n",
    "6. A more complete solution should build an agent with a set of tools that allows it to plan the sequence of actions appropriately and dynamically change towards alternative workflows. For example, iteratively refining search results in Google Scholar, adapting strategies for finding relevant authors, articles, and institutions according to the discipline, being more flexible in processing information in its various formats, and formatting the report appropriately for the user.\n",
    "\n",
    "# Notebook Structure:\n",
    "\n",
    "The notebook develops the approach to the problem as a linear sequence of steps and is structured in 6 cells. In addition to the usual ones for importing necessary libraries and configuring initial variables, the remaining cells are:\n",
    "- Functions for working with Bibtex\n",
    "- Functions for searching information on Google\n",
    "- Use of LangChain\n",
    "- Main execution loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bibtexparser import loads\n",
    "from serpapi import GoogleSearch\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment and local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "serpapi_key = ''\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# Paths\n",
    "path_bibtex_file = \"test-data/items2.bib\"\n",
    "path_download_files = 'downloaded_texts'\n",
    "\n",
    "# Model to use and fundamental parameters.\n",
    "# Temperature set to 0 to ensure result reproducibility.\n",
    "# Default model defined in LangChain is maintained. \n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibtex related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Bibtex file and return  title list\n",
    "def load_bibtex(file_path):\n",
    "  with open(file_path) as bibtex_file:\n",
    "    bib_db = loads(bibtex_file.read())\n",
    "  titles = []\n",
    "  for entry in bib_db.entries:    \n",
    "    title = entry.get(\"title\", \"\")    \n",
    "    titles.append(title)\n",
    "  return titles\n",
    "\n",
    "# Return PDF download url from title\n",
    "def get_url_from_title(title):\n",
    "    url = None\n",
    "    search_params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": f\"{title}\",\n",
    "        \"api_key\": serpapi_key,        \n",
    "        \"hl\": \"en\"\n",
    "    }\n",
    "    search = GoogleSearch(search_params)\n",
    "    results = search.get_dict()\n",
    "    if 'resources' in results['organic_results'][0]:\n",
    "        if 'PDF' in str(results['organic_results'][0]['resources'][0]):\n",
    "            url = results['organic_results'][0]['resources'][0]['link']\n",
    "    return url\n",
    "\n",
    "# Download pdf from url and return file path\n",
    "def download_document(url, folder, title):\n",
    "  # Clean and shortening of title\n",
    "  title = re.sub('[^a-zA-Z]', '', title)\n",
    "  title = title[:12]\n",
    "  file_name = title + '.pdf'\n",
    "  # Check folder existence before download\n",
    "  if not os.path.exists(folder):\n",
    "      os.makedirs(folder)\n",
    "  file_path = os.path.join(folder, file_name)\n",
    "  session = requests.Session()\n",
    "  response = session.get(url, stream=True, headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=True)\n",
    "  # If request succesfull (status code 200),\n",
    "  # download file\n",
    "  if response.status_code == 200:\n",
    "      print('Downloading...')  \n",
    "      with open(file_path, 'wb') as f:\n",
    "           for chunk in response.iter_content(4096):\n",
    "            f.write(chunk)\n",
    "      if os.path.isfile(file_path):\n",
    "            print('File downloaded successfully to '+ file_path + ' !')\n",
    "      else:\n",
    "            print('Download failed.')\n",
    "            return None\n",
    "      return file_path\n",
    "  else:\n",
    "      print('Download failed.')\n",
    "      return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SerpAPI related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return authors IDs, and string with titles and authors\n",
    "# from topics\n",
    "def get_ids_authors_articles(topic):\n",
    "    res = ''\n",
    "    authors_ids = []\n",
    "    search_params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": f\"{topic}\",\n",
    "        \"api_key\": serpapi_key,\n",
    "        \"num\": 10,\n",
    "        \"hl\": \"en\"\n",
    "    }\n",
    "    search = GoogleSearch(search_params)\n",
    "    results = search.get_dict()\n",
    "    results_organic = results[\"organic_results\"]\n",
    "    # Extract information from article\n",
    "    for result in results_organic:\n",
    "        title = result[\"title\"]\n",
    "        publication_info = result[\"publication_info\"]\n",
    "        if 'authors' in publication_info:\n",
    "            authors = [author[\"name\"] for author in publication_info[\"authors\"]]\n",
    "            first_author_id = result['publication_info']['authors'][0]['author_id']    \n",
    "            res += \"Title: \" + title + '/n'\n",
    "            res += \"Authors: \" + str(authors) + '/n'\n",
    "            authors_ids.append(first_author_id)        \n",
    "    return authors_ids, res\n",
    "\n",
    "\n",
    "# Return dict author:afiliation from IDs authors list\n",
    "def get_author_affiliations(author_ids):\n",
    "    affiliations = {}\n",
    "    for author_id in author_ids:\n",
    "        url = f\"https://serpapi.com/search.json?engine=google_scholar_author&author_id={author_id}&api_key={serpapi_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        if 'affiliations' in data[\"author\"]:\n",
    "            affiliations[author_id] = data[\"author\"][\"affiliations\"]\n",
    "        else:\n",
    "            affiliations[author_id] = None\n",
    "    return affiliations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for summarizing documents, extracting topics, and reporting on main authors, documents, and institutions per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return document abstract in document_path \n",
    "# using LLMChain\n",
    "def summarize_document(document_path):\n",
    "    loader = PyMuPDFLoader(document_path)\n",
    "    docs = loader.load() \n",
    "    text = '' \n",
    "    # Checks if  pdf returns usable string \n",
    "    for doc in docs:\n",
    "        text += doc.page_content\n",
    "    pattern = r'[a-zA-Z]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if bool(matches):\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        summary = chain.run(docs)\n",
    "        return summary\n",
    "    return None \n",
    "\n",
    "# Return topic from abstract using LLMChain\n",
    "def extract_topic(summary):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the abstract in this text extract its main topic: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    topic = chain.run(summary)\n",
    "    return topic\n",
    "\n",
    "# Returns top 5 authors from a string containing author \n",
    "# and article information using LLMChain to handle ambiguities \n",
    "# in names, repetitions, etc.\n",
    "def extract_main_authors(authors_and_articles):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the following text extract the top five authors. Each author in a new line: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    main_authors = chain.run(authors_and_articles)\n",
    "    return main_authors\n",
    "\n",
    "# Returns top 5 articles from a string containing author and article \n",
    "# information using LLMChain to handle repeated titles with slight \n",
    "# variations, etc.\n",
    "def extract_main_articles(authors_and_articles):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"From the following text extract the top five titles. Mention its authors: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    main_articles = chain.run(authors_and_articles)\n",
    "    return main_articles\n",
    "\n",
    "# Returns main centers and institutions for a given topic \n",
    "# based on authors' affiliations\n",
    "def report_main_centers_and_institutions(affiliations):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Report only institution names from the following text. Ignore personal names, titles and roles. Also ignore incomplete information. Each institution in a new line: {text}\",\n",
    ")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    top_centers = chain.run(str(affiliations))\n",
    "    return top_centers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of titles from Bibtex file at path_bibtex_file\n",
    "titles = load_bibtex(path_bibtex_file)\n",
    "\n",
    "for title in titles:\n",
    "    # Obtain URL for PDF download version of title\n",
    "    url = get_url_from_title(title)\n",
    "\n",
    "    # If PDF version cannot be obtained, move on to the next title\n",
    "    if url == None:\n",
    "        print('Impossible to find pdf of: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Download document\n",
    "    document_path = download_document(url, path_download_files, title)\n",
    "    \n",
    "    # If document cannot be downloaded, move on to the next title\n",
    "    if document_path == None:\n",
    "        print('Impossible to download pdf of: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Summarize document\n",
    "    summary = summarize_document(document_path)\n",
    "    \n",
    "    # If text cannot be extracted from document, move on to the next title\n",
    "    if summary == None:\n",
    "        print('Impossible to extract text from pdf: ' + title)\n",
    "        continue\n",
    "    \n",
    "    # Extract topic of the document\n",
    "    topic = extract_topic(summary)\n",
    "    \n",
    "    # Get information about main authors and articles in the topic\n",
    "    author_ids, authors_and_articles = get_ids_authors_articles(topic)\n",
    "    \n",
    "    # Obtain main authors\n",
    "    main_authors = extract_main_authors(authors_and_articles)\n",
    "    \n",
    "    # Obtain main articles\n",
    "    main_articles = extract_main_articles(authors_and_articles)\n",
    "   \n",
    "    # Get affiliations of each author\n",
    "    affiliations = get_author_affiliations(author_ids)\n",
    "    \n",
    "    # Extract main centers and institutions from affiliations\n",
    "    top_centers = report_main_centers_and_institutions(str(affiliations))\n",
    "   \n",
    "    # Print obtained information for this title\n",
    "    print('')\n",
    "    print(f'Title: {title}')\n",
    "    print('Abstract:')\n",
    "    print(summary)\n",
    "    print(topic)\n",
    "    print('Main authors in the topic: ')\n",
    "    print(main_authors)\n",
    "    print('')\n",
    "    print('Main documents in the topic: ')\n",
    "    print(main_articles)\n",
    "    print('')\n",
    "    print('Main centers and institutions: ')\n",
    "    print(top_centers)\n",
    "    print('')\n",
    "    print('--------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-llama-cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
